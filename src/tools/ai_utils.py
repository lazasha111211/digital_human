from typing import Dict, Any, Optional
import torch
import importlib

from transformers import BitsAndBytesConfig 

from torch.types import Device

# æ˜¯å¦å¯ç”¨é‡åŒ–ï¼ˆä»… GPU ç”Ÿæ•ˆï¼‰
USE_QUANTIZATION = True
# é‡åŒ–ä½æ•°ï¼ˆ4bit/8bitï¼Œä»… GPU ç”Ÿæ•ˆï¼‰
QUANT_BIT = 4

# è‡ªåŠ¨é€‚é… macOS MPS / Windows CUDA / CPU
def get_accelerator_device() -> Device:
    if torch.backends.mps.is_available() and torch.backends.mps.is_built():
        return torch.device("mps")  # macOS MPS
    elif torch.cuda.is_available():
        return torch.device("cuda:0")  # Windows è‹±ä¼Ÿè¾¾ CUDA
    else:
        return torch.device("cpu")
     


# ===================== æ ¸å¿ƒï¼šå‚æ•°æ”¯æŒæ€§æ ¡éªŒå‡½æ•° =====================
def check_param_support() -> dict:
    """
    æ ¡éªŒå½“å‰ç³»ç»Ÿå¯¹å„å‚æ•°çš„æ”¯æŒæ€§ï¼Œè¿”å›æ ¡éªŒç»“æœå­—å…¸
    è¿”å›ç»“æ„ï¼š{
        "device": "cuda"/"mps"/"cpu",
        "use_fp16": {"supported": bool, "reason": str},
        "use_deepspeed": {"supported": bool, "reason": str},
        "use_cuda_kernel": {"supported": bool, "reason": str}
    }
    """
    result = {
        "device": None,
        "use_fp16": {"supported": False, "reason": ""},
        "use_deepspeed": {"supported": False, "reason": ""},
        "use_cuda_kernel": {"supported": False, "reason": ""}
    }

    # 1. å…ˆæ£€æµ‹åŸºç¡€è®¾å¤‡
    if torch.cuda.is_available():
        result["device"] = "cuda"
        # è·å– GPU ä¿¡æ¯ï¼ˆç”¨äº FP16 æ ¡éªŒï¼‰
        gpu_name = torch.cuda.get_device_name(0)
        gpu_capability = torch.cuda.get_device_capability(0)  # (ç®—åŠ›ä¸»ç‰ˆæœ¬, æ¬¡ç‰ˆæœ¬)
    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():
        result["device"] = "mps"
    else:
        result["device"] = "cpu"

    # 2. æ ¡éªŒ use_fp16
    if result["device"] == "cuda":
        # CUDAï¼šç®—åŠ› â‰¥5.0 æ”¯æŒ FP16ï¼ˆå¦‚ GTX 10xx/RTX 20xx+ï¼‰
        if gpu_capability >= (5, 0):
            result["use_fp16"]["supported"] = True
            result["use_fp16"]["reason"] = f"CUDA GPU ({gpu_name}) ç®—åŠ› {gpu_capability[0]}.{gpu_capability[1]} æ”¯æŒ FP16"
        else:
            result["use_fp16"]["reason"] = f"CUDA GPU ({gpu_name}) ç®—åŠ› {gpu_capability[0]}.{gpu_capability[1]} < 5.0ï¼Œä¸æ”¯æŒ FP16"
    elif result["device"] == "mps":
        # MPSï¼šApple Silicon åŸç”Ÿæ”¯æŒ FP16
        result["use_fp16"]["supported"] = True
        result["use_fp16"]["reason"] = "MPS (Apple Silicon) æ”¯æŒ FP16 æ¨ç†"
    else:
        # CPUï¼šFP16 æ— æ”¶ç›Šä¸”æ˜“å‡ºé”™
        result["use_fp16"]["reason"] = "CPU ç¯å¢ƒä¸‹ FP16 æ— æ€§èƒ½æ”¶ç›Šï¼Œä¸”ç²¾åº¦ç¨³å®šæ€§å·®"

    # 3. æ ¡éªŒ use_deepspeed
    if result["device"] == "cuda":
        # æ£€æŸ¥ deepspeed åº“æ˜¯å¦å®‰è£…
        if importlib.util.find_spec("deepspeed") is not None:
            result["use_deepspeed"]["supported"] = True
            result["use_deepspeed"]["reason"] = "CUDA ç¯å¢ƒ + deepspeed åº“å·²å®‰è£…ï¼Œæ”¯æŒ DeepSpeed"
        else:
            result["use_deepspeed"]["reason"] = "CUDA ç¯å¢ƒä½†æœªå®‰è£… deepspeed åº“ï¼ˆpip install deepspeedï¼‰"
    else:
        result["use_deepspeed"]["reason"] = f"{result['device'].upper()} ç¯å¢ƒä¸æ”¯æŒ DeepSpeedï¼ˆä»… CUDA æ”¯æŒï¼‰"

    # 4. æ ¡éªŒ use_cuda_kernel
    if result["device"] == "cuda":
        # æ£€æŸ¥ CUDA ç¼–è¯‘ç¯å¢ƒï¼ˆç®€æ˜“ç‰ˆï¼šé€šè¿‡ torch æ£€æµ‹ CUDA ç‰ˆæœ¬æ˜¯å¦æœ‰æ•ˆï¼‰
        try:
            # å°è¯•æ‰§è¡Œç®€å• CUDA æ ¸æ“ä½œï¼ˆéªŒè¯ CUDA ç¼–è¯‘ç¯å¢ƒï¼‰
            torch.randn(1).cuda()
            result["use_cuda_kernel"]["supported"] = True
            result["use_cuda_kernel"]["reason"] = "CUDA ç¯å¢ƒæœ‰æ•ˆï¼Œæ”¯æŒ CUDA Kernel åŠ é€Ÿ"
        except Exception as e:
            result["use_cuda_kernel"]["reason"] = f"CUDA ç¯å¢ƒå¼‚å¸¸ï¼Œä¸æ”¯æŒ CUDA Kernelï¼š{str(e)[:50]}..."
    else:
        result["use_cuda_kernel"]["reason"] = f"{result['device'].upper()} ç¯å¢ƒä¸æ”¯æŒ CUDA Kernelï¼ˆä»… CUDA æ”¯æŒï¼‰"

    return result  

def get_model_kwargs(device: str) -> Dict[str, Any]:
    """
    æ ¹æ®è®¾å¤‡åŠ¨æ€ç”Ÿæˆæ¨¡å‹åŠ è½½å‚æ•°ï¼ˆæ ¸å¿ƒé€‚é…é€»è¾‘ï¼‰
    :param device: è®¾å¤‡å­—ç¬¦ä¸²ï¼ˆ"cuda"/"mps"/"cpu"ï¼‰
        å½“åŠ è½½é Hugging Face å®˜æ–¹å†…ç½®çš„è‡ªå®šä¹‰æ¨¡å‹ï¼ˆå¦‚ Qwenã€Baichuanã€LLaMA ç­‰ï¼‰æ—¶ï¼Œè¿™äº›æ¨¡å‹çš„é…ç½® / æ¶æ„ä»£ç å¹¶æœª
    å†…ç½®åœ¨ transformers åº“ä¸­ï¼Œè€Œæ˜¯å­˜å‚¨åœ¨æ¨¡å‹ä»“åº“çš„ modeling_xxx.py/configuration_xxx.py ç­‰æ–‡ä»¶é‡Œï¼ˆå³ã€Œè¿œç¨‹ä»£ç ã€ï¼‰ã€‚
    trust_remote_code=True è¡¨ç¤ºï¼šğŸ‘‰ å…è®¸ transformers ä»æ¨¡å‹ä»“åº“ä¸‹è½½å¹¶æ‰§è¡Œè¿™äº›è‡ªå®šä¹‰ä»£ç ï¼Œä»¥æ­£ç¡®åŠ è½½æ¨¡å‹æ¶æ„å’Œé…ç½®
    
    :return: æ¨¡å‹åŠ è½½å‚æ•°å­—å…¸
    """
    model_kwargs = {
        "trust_remote_code": True,
        "low_cpu_mem_usage": True,  # æ‰€æœ‰è®¾å¤‡éƒ½å¼€å¯ï¼Œé™ä½å†…å­˜å ç”¨
    }

    # -------------------- GPUï¼ˆCUDAï¼‰é€‚é… --------------------
    if device == "cuda":
        # å¯ä»¥é€‰æ‹©é‡åŒ–é…ç½®ï¼ˆ4bit/8bitï¼‰ï¼Œè¿™é‡Œé€šè¿‡å…¨å±€å˜é‡è®¾ç½®ä½¿ç”¨4bité‡åŒ–é…ç½®å’Œï¼Œ
        if USE_QUANTIZATION and QUANT_BIT in [4, 8]:
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=(QUANT_BIT == 4),
                load_in_8bit=(QUANT_BIT == 8),
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
            )
            model_kwargs.update({
                "quantization_config": quantization_config,
                "load_in_4bit": (QUANT_BIT == 4),
                "load_in_8bit": (QUANT_BIT == 8),
            })
        # ä¹Ÿå¯ä»¥é€‰æ‹©éé‡åŒ–é…ç½®
        else:
            model_kwargs.update({
                "quantization_config": None,
                "load_in_4bit": False,
                "load_in_8bit": False,
            })
        # GPU æœ€ä¼˜ dtypeï¼šbfloat16ï¼ˆæ”¯æŒçš„è¯ï¼‰/float16
        model_kwargs["torch_dtype"] = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
        model_kwargs["device_map"] = device

    # -------------------- MPSï¼ˆmacOSï¼‰é€‚é… --------------------
    elif device == "mps":
        model_kwargs.update({
            "quantization_config": None,  # MPS æš‚ä¸æ”¯æŒ BitsAndBytes é‡åŒ–
            "load_in_4bit": False,
            "load_in_8bit": False,
            "torch_dtype": torch.float16,  # MPS ç”¨ float16 æé€Ÿé™å†…å­˜
            "device_map": device
        })

    # -------------------- CPU é€‚é… --------------------
    else:  # cpu
        model_kwargs.update({
            "quantization_config": None,  # CPU é‡åŒ–æ— æ”¶ç›Š
            "load_in_4bit": False,
            "load_in_8bit": False,
            "torch_dtype": torch.float32,  # CPU ç”¨ float32 æ›´ç¨³å®š
            "device_map": device
        })

    return model_kwargs          



  