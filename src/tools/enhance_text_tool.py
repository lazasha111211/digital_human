import torch
import os
import warnings
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig 
from ai_utils import get_accelerator_device, get_model_kwargs # noqa: F403
# å½»åº•å±è”½æ— å…³è­¦å‘Šï¼ˆå¦‚å‚æ•°æç¤ºã€è®¾å¤‡é€‚é…ï¼‰
warnings.filterwarnings("ignore")
# ===================== å…¨å±€é…ç½®ï¼ˆå¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰ =====================
# æ¨¡å‹åç§°/è·¯å¾„
MODEL_NAME = "./checkpoints/Qwen2.5-1.5B-Instruct"

# æœ€å¤§ç”Ÿæˆé•¿åº¦
# MAX_NEW_TOKENS = 512
# æ¨ç†æ¸©åº¦
# TEMPERATURE = 0.7

# è£…è½½æ¨¡å‹
def load_qwen25_15b(model_dir: str = "./checkpoints/Qwen2.5-1.5B-Instruct"):
    """
    åŠ è½½ Qwen2.5-1.5B-Instruct:
    1. åŠ è½½æœ¬åœ°æ¨¡å‹
    2. é€šé…MPS(macOS)ã€CPUã€GPU(è‹±ä¼Ÿè¾¾ CUDA)
    3. å¦‚æœéœ€è¦è®¾ç½®å…¨å±€ç¼“å­˜å˜é‡ï¼Œå¯ä»¥åŠ å¿«åé¢æ“ä½œé€Ÿåº¦
    
    """
    local_model_dir = os.path.join(os.getcwd(), model_dir)
    print(f"model dir : {local_model_dir}")
  
    try:
        print("ğŸ“¥ åŠ è½½ Qwen2.5-1.5B-Instruct ...")
            
        # 1. åŠ è½½åˆ†è¯å™¨ï¼ˆQwen ä¸“å±ï¼Œéœ€ trust_remote_codeï¼‰
        TOKENIZER = AutoTokenizer.from_pretrained(
            local_model_dir,
            trust_remote_code=True,
            clean_up_tokenization_spaces=True
        )

        # 2. æ ¸å¿ƒé…ç½®ï¼šé€‚é…å…¨ç¯å¢ƒ
        device = get_accelerator_device()
        
        
        print(f"å½“å‰åŠ é€Ÿè®¾å¤‡ï¼š{device.type}")
        model_kwargs = get_model_kwargs(device.type)  
        

        # 3. åŠ è½½æ¨¡å‹ï¼ˆè‡ªåŠ è½½æœ¬åœ°ï¼‰
        MODEL = AutoModelForCausalLM.from_pretrained(
            local_model_dir,
            **model_kwargs
        ).eval()  # æ¨ç†æ¨¡å¼ï¼Œç¦ç”¨æ¢¯åº¦
        
        MODEL = MODEL.to(device)
        
    except Exception as e:
        raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)}")

    return TOKENIZER, MODEL

# è°ƒç”¨æ¨¡å‹ç”Ÿæˆæ–‡å­—
def qwen_generate(
    raw_text: str,
    requirements: str,
    model_dir: str = "./checkpoints/Qwen2.5-1.5B-Instruct",
    max_new_tokens: int = 512,
    temperature: float = 0.3
) -> str:
    """
    è°ƒç”¨ Qwen2.5-1.5B-Instruct ç”Ÿæˆæ–‡æœ¬ï¼š
    :param raw_text: åŸå§‹æ–‡å­—ï¼ˆä¸èƒ½ä¸ºç©ºï¼‰
    :param requirements: å¯¹æ–‡å­—çš„ä¿®æ”¹/ç”Ÿæˆè¦æ±‚ï¼ˆä¸èƒ½ä¸ºç©ºï¼‰
    :param model_dir: æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°/ Hugging Face è¿œç¨‹ï¼‰,æœ¬åœ°ç›¸å¯¹è·¯å¾„      
    :param max_new_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
    :param temperature: ç”Ÿæˆéšæœºæ€§(0-1,è¶Šå°è¶Šç¨³å®šï¼‰
    :return: ç¬¦åˆè¦æ±‚çš„æœ€ç»ˆæ–‡æœ¬
    """

    # åŠ è½½æ¨¡å‹
    tokenizer, model = load_qwen25_15b(model_dir)

    # æ„é€  Qwen ä¸“å± Prompt æ ¼å¼ï¼ˆå…³é”®ï¼šé€‚é…æ¨¡å‹æŒ‡ä»¤ç†è§£é€»è¾‘ï¼‰
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬å¤„ç†åŠ©æ‰‹ï¼Œä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·è¦æ±‚å¤„ç†æ–‡å­—ï¼Œç›´æ¥è¾“å‡ºæœ€ç»ˆç»“æœï¼Œä¸æ·»åŠ ä»»ä½•é¢å¤–è§£é‡Šã€æ ‡é¢˜æˆ–æ ¼å¼ã€‚"},
        {"role": "user", "content": f"åŸå§‹æ–‡å­—ï¼š{raw_text}\nè¦æ±‚ï¼š{requirements}"}
    ]
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # é…ç½®ç”Ÿæˆå‚æ•°ï¼ˆé€šè¿‡ GenerationConfig ç¡®ä¿å‚æ•°ç”Ÿæ•ˆï¼‰
    gen_config = GenerationConfig(
        max_new_tokens=max_new_tokens,
        temperature=temperature,
        top_p=0.9,
        repetition_penalty=1.1,
        do_sample=False,  # ç¡®å®šæ€§ç”Ÿæˆï¼Œä¸¥æ ¼æŒ‰è¦æ±‚è¾“å‡º
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id
    )

    # ç¼–ç è¾“å…¥
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=8192
    )

    device = get_accelerator_device()
   
    
    # å¼ é‡è¿ç§»åˆ°ç›®æ ‡è®¾å¤‡ï¼ˆå…¼å®¹æ‰€æœ‰è®¾å¤‡ç±»å‹ï¼‰ï¼šå®ç°ç¡¬ä»¶åŠ é€Ÿã€ä¿è¯è®¡ç®—ä¸€è‡´æ€§
    # æ¨¡å‹åœ¨ GPU ä¸Šï¼Œä½†è¾“å…¥å¼ é‡åœ¨ CPU ä¸Šï¼Œç›´æ¥æ¨ç†ä¼šæŠ›å‡º RuntimeError
    # æå‡é²æ£’æ€§ï¼ˆRobustnessï¼‰ï¼š æŒ‡ç¨‹åº / ç³»ç»Ÿåœ¨é¢å¯¹ã€Œå¼‚å¸¸è¾“å…¥ã€ç¡¬ä»¶ / ç¯å¢ƒæ³¢åŠ¨ã€è¾¹ç•Œåœºæ™¯ã€æ—¶ï¼Œ
    # ä»èƒ½ä¿æŒç¨³å®šè¿è¡Œã€ä¸å´©æºƒï¼Œä¸”èƒ½åˆç†å¤„ç†é”™è¯¯çš„èƒ½åŠ›ã€‚ç®€å•æ¥è¯´ï¼šé²æ£’çš„ä»£ç ä¸æ€• â€œæ„å¤–â€ï¼Œèƒ½æ‰›ä½å„ç§ â€œéæ­£å¸¸æƒ…å†µâ€ã€‚
    try:
        inputs = inputs.to(device)
    except RuntimeError as e:
        if "MPS" in str(e):
            warnings.warn(f"MPS è¿ç§»å¤±è´¥,é™çº§åˆ°CPU:{e}")
            device = torch.device("cpu")
            inputs = inputs.to(device)
        else:
            raise

    # ç”Ÿæˆæ–‡æœ¬ï¼ˆæ— æ¢¯åº¦ï¼ŒèŠ‚çœå†…å­˜ï¼‰
    # torch.no_grad() æ˜¯ PyTorch çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œè¿›å…¥è¯¥ä¸Šä¸‹æ–‡åï¼š
    #    ç¦ç”¨æ‰€æœ‰å¼ é‡çš„æ¢¯åº¦è®¡ç®—ï¼šæ¨¡å‹å‰å‘ä¼ æ’­æ—¶ï¼Œä¸å†è®°å½•æ¢¯åº¦ï¼ˆrequires_grad=Falseï¼‰ï¼›
    #    èŠ‚çœæ˜¾å­˜ / å†…å­˜ï¼šæ¢¯åº¦ä¿¡æ¯ä¼šå ç”¨å¤§é‡æ˜¾å­˜ï¼ˆå°¤å…¶æ˜¯å¤§æ¨¡å‹ï¼‰ï¼Œç¦ç”¨åå¯å‡å°‘ 30%+ æ˜¾å­˜å ç”¨ï¼›
    #    æå‡æ¨ç†é€Ÿåº¦ï¼šæ— éœ€è®¡ç®— / å­˜å‚¨æ¢¯åº¦ï¼Œå‰å‘è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚
    #    å¤§æ¨¡å‹ç”Ÿæˆï¼ˆæ¨ç†ï¼‰æ˜¯ã€Œåªè¯»ã€è¿‡ç¨‹ï¼Œä¸éœ€è¦åå‘ä¼ æ’­æ›´æ–°å‚æ•°ï¼Œæ¢¯åº¦è®¡ç®—å®Œå…¨æ˜¯å†—ä½™å¼€é”€

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            generation_config=gen_config
        )

    # è§£ç å¹¶æ¸…ç†ç»“æœï¼ˆä»…ä¿ç•™ç”Ÿæˆçš„å†…å®¹ï¼Œå‰”é™¤ Promptï¼‰
    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    final_result = output_text.replace(prompt, "").strip()

    
    # æ¸…ç†ç©ºè¡Œå’Œå†—ä½™å†…å®¹
    #next_text = "\n".join([line.strip() for line in final_result.split("\n") if line.strip()])
    
    
    final_result = final_result.split("\nassistant\n")[1]
    
  
    
    return final_result